{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scrap.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOGbS6WyufhV4386tkzeycu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaviR08/Ravi-Patel/blob/master/scrap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GaKC8Wwh1S3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import time\n",
        "import re\n",
        "import logging\n",
        "import traceback\n",
        "from datetime import datetime, timedelta\n",
        "import argparse\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io \n",
        "\n",
        "GM_WEBPAGE = 'https://www.google.com/maps/'\n",
        "MAX_WAIT = 10\n",
        "MAX_RETRY = 10\n",
        "MAX_SCROLLS = 40\n",
        "\n",
        "class GoogleMapsScraper:\n",
        "\n",
        "    def __init__(self, debug=False):\n",
        "        self.debug = debug\n",
        "        self.driver = self.__get_driver()\n",
        "        self.logger = self.__get_logger()\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, tb):\n",
        "        if exc_type is not None:\n",
        "            traceback.print_exception(exc_type, exc_value, tb)\n",
        "\n",
        "        self.driver.close()\n",
        "        self.driver.quit()\n",
        "\n",
        "        return True\n",
        "\n",
        "    def sort_by_date(self, url):\n",
        "        self.driver.get(url)\n",
        "        wait = WebDriverWait(self.driver, MAX_WAIT)\n",
        "\n",
        "        # open dropdown menu\n",
        "        clicked = False\n",
        "        tries = 0\n",
        "        while not clicked and tries < MAX_RETRY:\n",
        "            try:\n",
        "                if not self.debug:\n",
        "                    menu_bt = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'div.cYrDcjyGO77__container')))\n",
        "                else:\n",
        "                    menu_bt = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@data-value=\\'Sort\\']')))\n",
        "                menu_bt.click()\n",
        "\n",
        "                clicked = True\n",
        "                time.sleep(3)\n",
        "            except Exception as e:\n",
        "                tries += 1\n",
        "                self.logger.warn('Failed to click recent button')\n",
        "\n",
        "            # failed to open the dropdown\n",
        "            if tries == MAX_RETRY:\n",
        "                return -1\n",
        "\n",
        "        # second element of the list: most recent\n",
        "        recent_rating_bt = self.driver.find_elements_by_xpath('//li[@role=\\'menuitemradio\\']')[1] \n",
        "        recent_rating_bt.click()\n",
        "\n",
        "        # wait to load review (ajax call)\n",
        "        time.sleep(5)\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def get_reviews(self, offset):\n",
        "\n",
        "        # scroll to load reviews\n",
        "        self.__scroll()\n",
        "\n",
        "        # wait for other reviews to load (ajax)\n",
        "        time.sleep(4)\n",
        "\n",
        "        # expand review text\n",
        "        self.__expand_reviews()\n",
        "\n",
        "        # parse reviews\n",
        "        response = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "        rblock = response.find_all('div', class_='section-review-content')\n",
        "        parsed_reviews = []\n",
        "        for index, review in enumerate(rblock):\n",
        "            if index >= offset:\n",
        "                parsed_reviews.append(self.__parse(review))\n",
        "\n",
        "        return parsed_reviews\n",
        "\n",
        "\n",
        "    def get_account(self, url):\n",
        "\n",
        "        self.driver.get(url)\n",
        "\n",
        "        # ajax call also for this section\n",
        "        time.sleep(4)\n",
        "\n",
        "        resp = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "\n",
        "        place_data = self.__parse_place(resp)\n",
        "\n",
        "        return place_data\n",
        "\n",
        "\n",
        "    def __parse(self, review):\n",
        "\n",
        "        item = {}\n",
        "\n",
        "        id_review = review.find('button', class_='section-review-action-menu')['data-review-id']\n",
        "        username = review.find('div', class_='section-review-title').find('span').text\n",
        "\n",
        "        try:\n",
        "            review_text = self.__filter_string(review.find('span', class_='section-review-text').text)\n",
        "        except Exception as e:\n",
        "            review_text = None\n",
        "\n",
        "        rating = float(review.find('span', class_='section-review-stars')['aria-label'].split(' ')[1])\n",
        "        relative_date = review.find('span', class_='section-review-publish-date').text\n",
        "\n",
        "        try:\n",
        "            n_reviews_photos = review.find('div', class_='section-review-subtitle').find_all('span')[1].text\n",
        "            metadata = n_reviews_photos.split('\\xe3\\x83\\xbb')\n",
        "            if len(metadata) == 3:\n",
        "                n_photos = int(metadata[2].split(' ')[0].replace('.', ''))\n",
        "            else:\n",
        "                n_photos = 0\n",
        "\n",
        "            idx = len(metadata)\n",
        "            n_reviews = int(metadata[idx - 1].split(' ')[0].replace('.', ''))\n",
        "\n",
        "        except Exception as e:\n",
        "            n_reviews = 0\n",
        "            n_photos = 0\n",
        "\n",
        "        user_url = review.find('a')['href']\n",
        "\n",
        "        item['id_review'] = id_review\n",
        "        item['caption'] = review_text\n",
        "\n",
        "        # depends on language, which depends on geolocation defined by Google Maps\n",
        "        # custom mapping to transform into date shuold be implemented\n",
        "        item['relative_date'] = relative_date\n",
        "\n",
        "        # store datetime of scraping and apply further processing to calculate\n",
        "        # correct date as retrieval_date - time(relative_date)\n",
        "        item['retrieval_date'] = datetime.now()\n",
        "        item['rating'] = rating\n",
        "        item['username'] = username\n",
        "        item['n_review_user'] = n_reviews\n",
        "        item['n_photo_user'] = n_photos\n",
        "        item['url_user'] = user_url\n",
        "\n",
        "        return item\n",
        "\n",
        "\n",
        "    def __parse_place(self, response):\n",
        "\n",
        "        place = {}\n",
        "        place['overall_rating'] = float(response.find('div', class_='gm2-display-2').text.replace(',', '.'))\n",
        "        place['n_reviews'] = int(response.find('div', class_='gm2-caption').text.replace('.', '').replace(',','').split(' ')[0])\n",
        "\n",
        "        return place\n",
        "\n",
        "    # expand review description\n",
        "    def __expand_reviews(self):\n",
        "        # use XPath to load complete reviews\n",
        "        links = self.driver.find_elements_by_xpath('//button[@class=\\'section-expand-review blue-link\\']')\n",
        "        for l in links:\n",
        "            l.click()\n",
        "        time.sleep(2)\n",
        "\n",
        "\n",
        "    def __scroll(self):\n",
        "        scrollable_div = self.driver.find_element_by_css_selector('div.section-layout.section-scrollbox.scrollable-y.scrollable-show')\n",
        "        self.driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
        "\n",
        "\n",
        "    def __get_logger(self):\n",
        "        # create logger\n",
        "        logger = logging.getLogger('googlemaps-scraper')\n",
        "        logger.setLevel(logging.DEBUG)\n",
        "\n",
        "        # create console handler and set level to debug\n",
        "        fh = logging.FileHandler('gm-scraper.log')\n",
        "        fh.setLevel(logging.DEBUG)\n",
        "\n",
        "        # create formatter\n",
        "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "        # add formatter to ch\n",
        "        fh.setFormatter(formatter)\n",
        "\n",
        "        # add ch to logger\n",
        "        logger.addHandler(fh)\n",
        "\n",
        "        return logger\n",
        "\n",
        "\n",
        "    def __get_driver(self, debug=False):\n",
        "        options = Options()\n",
        "        if not self.debug:\n",
        "            options.add_argument(\"--headless\")\n",
        "        options.add_argument(\"--window-size=1366,768\")\n",
        "        options.add_argument(\"--disable-notifications\")\n",
        "        options.add_experimental_option('prefs', {'intl.accept_languages': 'en_GB'})\n",
        "        input_driver = webdriver.Chrome(executable_path='C:/Users/MAHESH PATEL/Downloads/chromedriver_win32')\n",
        "\n",
        "        return input_driver\n",
        "\n",
        "\n",
        "    # util function to clean special characters\n",
        "    def __filter_string(self, str):\n",
        "        strOut = str.replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' ')\n",
        "        return strOut\n",
        "\n",
        "HEADER = ['id_review', 'caption', 'timestamp', 'rating', 'username', 'n_review_user', 'n_photo_user', 'url_user']\n",
        "HEADER_W_SOURCE = ['id_review', 'caption', 'timestamp', 'rating', 'username', 'n_review_user', 'n_photo_user', 'url_user', 'url_source']\n",
        "\n",
        "def csv_writer(source_field, path='C:/Downloads/googlemaps-scraper-master/googlemaps-scraper-master/data/', outfile='gm_reviews.csv'):\n",
        "    targetfile = open(path + outfile, mode='w', encoding='utf-8', newline='\\n')\n",
        "    writer = csv.writer(targetfile, quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "    if source_field:\n",
        "        h = HEADER_W_SOURCE\n",
        "    else:\n",
        "        h = HEADER\n",
        "    writer.writerow(h)\n",
        "\n",
        "    return writer\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Google Maps reviews scraper.')\n",
        "    parser.add_argument('--N', type=int, default=100, help='Number of reviews to scrape')\n",
        "    parser.add_argument('--i', type=str, default='urls.txt', help='target URLs file')\n",
        "    parser.add_argument('--place', dest='place', action='store_true', help='Scrape place metadata')\n",
        "    parser.add_argument('--debug', dest='debug', action='store_true', help='Run scraper using browser graphical interface')\n",
        "    parser.add_argument('--source', dest='source', action='store_true', help='Add source url to CSV file (for multiple urls in a single file)')\n",
        "    parser.set_defaults(place=False, debug=False, source=False)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # store reviews in CSV file\n",
        "    writer = csv_writer(args.source)\n",
        "\n",
        "    with GoogleMapsScraper(debug=args.debug) as scraper:\n",
        "        with open(args.i, 'r') as urls_file:\n",
        "            for url in urls_file:\n",
        "\n",
        "                if args.place:\n",
        "                    print(scraper.get_account(url))\n",
        "                else:\n",
        "                    error = scraper.sort_by_date(url)\n",
        "                    if error == 0:\n",
        "\n",
        "                        n = 0\n",
        "                        while n < args.N:\n",
        "                            reviews = scraper.get_reviews(n)\n",
        "\n",
        "                            for r in reviews:\n",
        "                                row_data = list(r.values())\n",
        "                                if args.source:\n",
        "                                    row_data.append(url)\n",
        "\n",
        "                                writer.writerow(row_data)\n",
        "\n",
        "                            n += len(reviews)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}